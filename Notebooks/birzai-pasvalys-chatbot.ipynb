{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the OpenAI API key\n",
    "openai_key = os.getenv('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# arr = [\"Kos\", \"Kreta\", \"Rodas\"]\n",
    "arr = [\"Kos\"]\n",
    "merged_documents =[]\n",
    "\n",
    "for i in arr:\n",
    "  file_path = \"./data/\"+i+\".pdf\"\n",
    "  loader = PyPDFLoader(file_path)\n",
    "  document = loader.load()\n",
    "  merged_documents += document\n",
    "\n",
    "print(merged_documents[3].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(merged_documents)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### My Inserted code  - Do not run###\n",
    "\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain.vectorstores import FAISS  # or Chroma, or other vector stores\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def create_vectorstore(documents, embedding_model_name='all-MiniLM-L6-v2'):\n",
    "    \"\"\"Creates a vector store from documents.\"\"\"\n",
    "    try:\n",
    "        embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)\n",
    "        vectorstore = FAISS.from_documents(documents, embedding_function)\n",
    "        return vectorstore\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating vector store: {e}\")\n",
    "        return None\n",
    "\n",
    "def query_vectorstore(vectorstore, query_text, k=5):\n",
    "    \"\"\"Queries the vector store and prints the results.\"\"\"\n",
    "    try:\n",
    "        if vectorstore:\n",
    "            results = vectorstore.similarity_search(query_text, k=k)\n",
    "            if results:\n",
    "                for doc in results:\n",
    "                    print(doc.page_content)\n",
    "            else:\n",
    "                print(\"No results found.\")\n",
    "        else:\n",
    "            print(\"Vector store is not initialized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying vector store: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = ['documentas-1.docx', 'documentas-2.docx']\n",
    "\n",
    "    # Load and split documents\n",
    "    \n",
    "    # Create vector store\n",
    "    vectorstore = create_vectorstore(all_splits)\n",
    "\n",
    "    # Query the vector store\n",
    "    query_vectorstore(vectorstore, \"Kuriame mieste daugiau ežerų?\", k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key=openai_key, base_url=\"https://models.inference.ai.azure.com\")\n",
    "\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# visas procesas iki šios vietos vadinamas ingestion pipeline. Tai yra dokumentų paruošimas analizei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=openai_key, base_url=\"https://models.inference.ai.azure.com\", model=\"gpt-4o\")\n",
    "\n",
    "question = \"Kuo įdomus krokosas?\"\n",
    "\n",
    "retrieved_docs = vector_store.similarity_search(question)\n",
    "docs_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "promptAnswer = prompt.invoke({\"question\": question, \"context\": docs_content})\n",
    "answer = llm.invoke(promptAnswer)\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
